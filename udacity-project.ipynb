{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Create-an-Experiment-in-Azure-ML-workspace\" data-toc-modified-id=\"Create-an-Experiment-in-Azure-ML-workspace-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Create an Experiment in Azure ML workspace</a></span></li><li><span><a href=\"#HyperDrive-Pipeline\" data-toc-modified-id=\"HyperDrive-Pipeline-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>HyperDrive Pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Resources-for-Training-Experiments\" data-toc-modified-id=\"Create-Resources-for-Training-Experiments-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create Resources for Training Experiments</a></span></li><li><span><a href=\"#Hyperparameter-Tunning\" data-toc-modified-id=\"Hyperparameter-Tunning-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Hyperparameter Tunning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Parameter-sampler\" data-toc-modified-id=\"Parameter-sampler-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Parameter sampler</a></span></li><li><span><a href=\"#Early-Termination-Policy\" data-toc-modified-id=\"Early-Termination-Policy-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Early Termination Policy</a></span></li><li><span><a href=\"#Create-a-SKLearn-Estimator\" data-toc-modified-id=\"Create-a-SKLearn-Estimator-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Create a SKLearn Estimator</a></span></li><li><span><a href=\"#Create-a-HyperDriveConfig\" data-toc-modified-id=\"Create-a-HyperDriveConfig-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Create a <a href=\"https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py\" target=\"_blank\">HyperDriveConfig</a></a></span></li></ul></li></ul></li><li><span><a href=\"#AutoML-Run\" data-toc-modified-id=\"AutoML-Run-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>AutoML Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Dataset\" data-toc-modified-id=\"Create-Dataset-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Create Dataset</a></span></li><li><span><a href=\"#Inspect-Dataset\" data-toc-modified-id=\"Inspect-Dataset-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Inspect Dataset</a></span></li><li><span><a href=\"#Clean-and-Split-Dataset\" data-toc-modified-id=\"Clean-and-Split-Dataset-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Clean and Split Dataset</a></span></li><li><span><a href=\"#Configure-Experiment\" data-toc-modified-id=\"Configure-Experiment-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Configure Experiment</a></span></li><li><span><a href=\"#Submitting-Training-Experiment\" data-toc-modified-id=\"Submitting-Training-Experiment-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Submitting Training Experiment</a></span></li><li><span><a href=\"#Monitor-using-Widget\" data-toc-modified-id=\"Monitor-using-Widget-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Monitor using <code>Widget</code></a></span></li><li><span><a href=\"#Retrieve-and-Save-Best-Model\" data-toc-modified-id=\"Retrieve-and-Save-Best-Model-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Retrieve and Save Best Model</a></span></li></ul></li><li><span><a href=\"#Cleaning-Up-Cluster\" data-toc-modified-id=\"Cleaning-Up-Cluster-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Cleaning Up Cluster</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Experiment in Azure ML workspace\n",
    "\n",
    "For this project we will be using an Azure Machine Learning Notebook VM, therefore we can skip setting up the environment.\n",
    "\n",
    "To start we need to initialize our workspace and create a Azule ML experiment. It is also to remember that accessing the Azure ML workspace requires authentication with Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "# Initialize a workspace object for an existing Azure Machine Learning Workspace\n",
    "ws = Workspace.get(\"quick-starts-ws-127535\")\n",
    "\n",
    "# Create a experiment\n",
    "exp = Experiment(workspace=ws, name=\"udacity-project\")\n",
    "\n",
    "run = exp.start_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dic_data = {'Workspace name': ws.name,\n",
    "            'Azure region': ws.location,\n",
    "            'Subscription id': ws.subscription_id,\n",
    "            'Resource group': ws.resource_group,\n",
    "            'Experiment Name': exp.name}\n",
    "\n",
    "df_data = pd.DataFrame.from_dict(data = dic_data, orient='index')\n",
    "\n",
    "df_data.rename(columns={0:''}, inplace = True)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make it in my own way\n",
    "import pandas as pd\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperDrive Pipeline\n",
    "\n",
    "## Create Resources for Training Experiments\n",
    "\n",
    "Now that we have initialized our workspace and created our experiment, it is time to define our resources.\n",
    "\n",
    "In this section you will create default compute clusters for use by the notebook and any other necessary operations we need.\n",
    "\n",
    "In order to create a cluster we need to specify a compute configuration that defines the `type of machine` to be used and the `scalability behaviors`. Also, it is necessary to define the name of the cluster which must be unique within the workspace. This name is used to address the cluster later.\n",
    "\n",
    "For this project we use a CPU cluster with following parameters:\n",
    "\n",
    "* `type of the machine`:\n",
    "\n",
    "    * `vm_size`: Defines the size of the virtual machine. We use here \"STANDARD_D2_V2\" (more details [here](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series))\n",
    "\n",
    "* `Scalability behaviors`:\n",
    "\n",
    "    * `min_nodes`: Sets minimun size of the cluster. Setting the minimum to 0 the cluster will shut down all nodes while not in use. If you use another value you are able to have faster start-up times, but you will also be billed when the cluster is not in use.\n",
    "\n",
    "    * `max_nodes`: Sets the maximun size of the cluster. Larger number allows for more concurrency and a greater distributed processing of scale-out jobs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Define CPU cluster name\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print(\"Found existing cpu-cluster\")\n",
    "except ComputeTargetException:\n",
    "    \n",
    "    # Specify the configuration for the new cluster\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\",\n",
    "                                                           min_nodes=0, # when innactive\n",
    "                                                           max_nodes=4) # when busy\n",
    "\n",
    "    # Create the cluster with the specified name and configuration\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    \n",
    "    # Wait for the cluster to complete, show the output log\n",
    "    cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598275788675
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Check details about compute_targets (i.e. cpu_cluster)\n",
    "\n",
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning\n",
    "\n",
    "### Parameter sampler\n",
    "\n",
    "In this example using HyperDrive we use [`random sampling`](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.randomparametersampling?view=azure-ml-py) to try different configuration sets of hyperparameters to maximize the chosen primary metric, accuracy. The function `choice` specify a discrete set of options to sample from.\n",
    "\n",
    "The hyperparameters and metric used are defined in the script `train.py`.\n",
    "\n",
    "### Early Termination Policy\n",
    "\n",
    "This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric.\n",
    "\n",
    "An early termination policy help us improving computational efficiency by terminating poorly performing runs.\n",
    "\n",
    "The `early termination policy` we used [`Bandit Policy`]( https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?preserve-view=true&view=azure-ml-py#&preserve-view=truedefinition ). This policy is based on `slack factor/slack amount` and `evaluation interval`. Bandit terminates runs where the primary metric is not within the specified slack factor/slack amount compared to the best performing run.\n",
    "\n",
    "This allows more aggressive savings than Median Stopping policy if we apply a smaller allowable slack.\n",
    "\n",
    "Parameter `slack_factor` which is the slack allowed with respect to the best performing training run, need to be defined while `evaluation_interval` and `delay_interval` are optional.\n",
    "\n",
    "`evaluation_interval` says when the policy is applied. If the `evaluation_interval` is not defined the default value is one, i.e., policy is applied every time the training script reports the primary metric.\n",
    "\n",
    "Specifying `delay_interval` avoids premature termination of training runs by allowing all configurations to run for a minimum number of intervals. If specified, the policy applies every multiple of evaluation_interval that is greater than or equal to delay_evaluation.\n",
    "\n",
    "For example, in our example, by applying the Bandit policy with `slack_factor = 0.1`, `evaluation_interval=2`, `delay_evaluation=5` the early termination policy is applied at every other time interval when metrics are reported, starting at evaluation interval 5. Any run whose primary metric falls outside of the top 10% range, Azure ML terminate the job.\n",
    "\n",
    "### Create a SKLearn Estimator\n",
    "\n",
    "[SKLearn Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.sklearn.sklearn?view=azure-ml-py) creates an estimator for training in Scikit-learn experiments.\n",
    "\n",
    "### Create a [HyperDriveConfig](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)\n",
    "\n",
    "Now we are ready to configure a run configuration object. \n",
    "\n",
    "As parameters we inform `parameter sampler`, `early termination policy`, and `estimator` that we just configured. We also specify the primary metric `Accuracy` that's recorded in your training runs and we tell the service that we want to maximize this value.  \n",
    "\n",
    "Moreover, we set the `number of samples` to 20, and `maximal concurrent job` to 4, which is the same as the number of nodes in our computer cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform, choice\n",
    "import os\n",
    "\n",
    "# Specify parameter sampler\n",
    "\n",
    "ps = RandomParameterSampling({\n",
    "    '--C': choice(0.01, 0.1, 0.2, 0.5, 0.7, 1.0),\n",
    "    '--max_iter': choice(range(10,110,10))\n",
    "    }\n",
    ")\n",
    "\n",
    "# Specify a Policy\n",
    "policy = BanditPolicy(slack_factor = 0.1, # specifies the allowable slack as a ratio\n",
    "                      evaluation_interval=2, # frequency for applying the policy\n",
    "                      delay_evaluation=5) # delays the first policy evaluation for a specified number of intervals\n",
    "\n",
    "if \"training\" not in os.listdir():\n",
    "    os.mkdir(\"./training\")\n",
    "\n",
    "# Create a SKLearn estimator for use with train.py\n",
    "\n",
    "est = SKLearn( \n",
    "    source_directory='./', # directory containing experiment configuration files (train.py)\n",
    "    compute_target=cpu_cluster, # compute target where training will happen\n",
    "    vm_size=\"STANDARD_D2_V2\", # VM size of the compute target\n",
    "    vm_priority='lowpriority', # VM priority of the compute target (default value is 'dedicated')\n",
    "    entry_script='train.py'\n",
    ")\n",
    "\n",
    "# Create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(estimator=est,\n",
    "                                hyperparameter_sampling=ps,\n",
    "                                policy=policy,\n",
    "                                primary_metric_name='Accuracy',\n",
    "                                primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                max_total_runs=4,\n",
    "                                max_concurrent_runs=4\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit hyperdrive run to the experiment \n",
    "\n",
    "hyperdrive_run = exp.submit(config = hyperdrive_config)\n",
    "\n",
    "# Show run details with the Jupyter widget\n",
    "\n",
    "RunDetails(hyperdrive_run).show()\n",
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Get your best run and save the model from that run.\n",
    "\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print('Accuracy:', best_run_metrics['Accuracy'])\n",
    "\n",
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check metrics details\n",
    "\n",
    "best_run_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name of files of best_run\n",
    "best_run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model, i.e., output file of best_run\n",
    "model = best_run.register_model(model_name='model_hd', model_path='outputs/model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Run\n",
    "\n",
    "Now we use the same dataset to obtain a model by running AutoML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "\n",
    "# Create TabularDataset using TabularDatasetFactory\n",
    "\n",
    "ds = TabularDatasetFactory.from_delimited_files(path = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with ds data\n",
    "\n",
    "ds_df = ds.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains 32950 entries and 21 columns where one of the column is our target (`y`). This dataset is a bit imbalanced, therefore I'm applying stratify when spliting the data so we can have the same percentage of each class in train and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import clean_data\n",
    "\n",
    "# Use the clean_data function to clean your data.\n",
    "x, y = clean_data(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test sets - \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=123, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with only training data\n",
    "\n",
    "df_train = pd.concat([x_train,y_train], axis=1)\n",
    "df_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_primary_metrics('classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598275665403
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "# automl_settings = {\n",
    "# #    \"enable_early_stopping\" : True,\n",
    "# #     \"iteration_timeout_minutes\": 5,\n",
    "# #     \"max_concurrent_iterations\": 4, default = 1 (AmlCompute clusters support one interation running per node)\n",
    "#     \"max_cores_per_iteration\": -1, # all the possible cores per iteration per child-run.\n",
    "#     #\"n_cross_validations\": 5,\n",
    "#     \"primary_metric\": 'accuracy',\n",
    "#     \"featurization\": 'auto',\n",
    "#     \"verbosity\": logging.INFO,\n",
    "# }\n",
    "\n",
    "# automl_config = AutoMLConfig(\n",
    "#     experiment_timeout_minutes=30,\n",
    "#     task=\"classification\",\n",
    "#     training_data=df_train,\n",
    "#     label_column_name='y',\n",
    "#     **automl_settings\n",
    "# )\n",
    "\n",
    "# Set parameters for AutoMLConfig\n",
    "# NOTE: DO NOT CHANGE THE experiment_timeout_minutes PARAMETER OR YOUR INSTANCE WILL TIME OUT.\n",
    "# If you wish to run the experiment longer, you will need to run this notebook in your own\n",
    "# Azure tenant, which will incur personal costs.\n",
    "\n",
    "automl_config = AutoMLConfig(\n",
    "    task=\"classification\",\n",
    "    experiment_timeout_minutes=30,\n",
    "    compute_target=cpu_cluster,\n",
    "    primary_metric=\"accuracy\",\n",
    "    training_data=df_train,\n",
    "    label_column_name='y',\n",
    "    n_cross_validations=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Training Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Submit your automl run\n",
    "\n",
    "experiment_name = 'automl-experiment'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "automl_run = experiment.submit(config=automl_config, show_output=True)\n",
    "automl_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor using `Widget`\n",
    "\n",
    "Once more we make use of `widget`. This time to explore the results obtained by using AutoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.widgets import RunDetails\n",
    "RunDetails(automl_run).show()\n",
    "\n",
    "automl_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Save Best Model\n",
    "\n",
    "Below we select the best model from all the training iterations using get_output method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model\n",
    "\n",
    "best_run, fitted_model = automl_run.get_output()\n",
    "\n",
    "# get name of files of best_run\n",
    "best_run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "best_automl_run.register_model(model_name = \"automl.pkl\", model_path = './outputs/')\n",
    "print(best_model._final_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.steps\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Up Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
